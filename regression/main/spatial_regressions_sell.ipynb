{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Impact of Remote Work on São Paulo's Real Estate Market: A Spatial Regression Analysis of Price Dynamics\n",
    "This notebook has the objective of analyzing the impact of remote work on São Paulo's real estate market. The analysis focuses on the relationship between the teleworkability index and property prices in different districts of São Paulo. The study uses spatial regression models to account for spatial dependencies and commuting patterns between districts. The analysis is conducted for the years 2018 to 2021 to understand how the relationship between teleworkability and property prices has evolved over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and Loading Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Installing and Loading Required Packages\n",
    "This section installs and loads essential R packages for data analysis and visualization. The `stargazer` package is used for creating well-formatted tables. The `spatialreg` package is utilized for spatial regression analysis, and the `sf` package is used for spatial data manipulation. The `flextable` package is used for creating tables. The `spdep` package offers spatial dependence methods. The `tmap` package is used for thematic mapping, and `corrr` is for exploring correlations. The `ggplot2` package is used for visualization, and `ggcorrplot` creates correlation plots. The `car` package offers regression diagnostics, and `HH` provides additional statistical methods. Both `gridExtra` and `cowplot` help arrange multiple plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#install.packages(c(\"stargazer\", \"spatialreg\", \"flextable\", \"spdep\", \"corrr\", \"ggplot2\", \"ggcorrplot\", \"car\", \"HH\", \"gridExtra\", \"cowplot\", \"dplyr\"))\n",
    "library(spatialreg) #for spatial regression analysis\n",
    "library(sf) #for spatial data manipulation\n",
    "library(spdep) #for spatial dependence methods\n",
    "library(corrr) #for exploring correlations\n",
    "library(ggplot2) #for visualization\n",
    "library(ggcorrplot) #for correlation plots\n",
    "library(car) #for regression diagnostics\n",
    "library(HH) #for additional statistical methods\n",
    "library(dplyr) #for filter the data by year and remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Data Preparation and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Reading the Main Data Frame\n",
    "Reading the data from the CSV file and displaying the first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data_sell <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/web-scraper/resources/clean-data/sell/sell_2018_2021.csv\", dec = \".\", header = TRUE, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Convert categorical variables to factors (dummy variables)\n",
    "Converting categorical variables to factors to use them in the regression model. The variables `gym`, `field_quadra`, `elevator`, `furnished`, and `swimming_pool` are dummy variables and thus need to be converted to factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Factorizing the categorical variables\n",
    "data_sell$gym <- as.factor(data_sell$gym)\n",
    "data_sell$field_quadra <- as.factor(data_sell$field_quadra)\n",
    "data_sell$elevator <- as.factor(data_sell$elevator)\n",
    "data_sell$furnished <- as.factor(data_sell$furnished)\n",
    "data_sell$swimming_pool <- as.factor(data_sell$swimming_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Convert date to date format\n",
    "Converting the date column to a date format to filter the data by year. The date column is in the format `dd/mm/yyyy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "data_sell$date <- as.Date(data_sell$date, format = \"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Filter the data by year\n",
    "Filtering the data by year allows us to run the regression model for each year studied. We can understand the relationship between the variables over time by running the regression for each year. The study will focus on understanding how `price_m2` behaves with the variation of the `teleworkable` index over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Filtering the data by year to run the regression model for each year studied\n",
    "data_sell_2018 <- filter(data_sell, year == 2018)\n",
    "data_sell_2019 <- filter(data_sell, year == 2019)\n",
    "data_sell_2020 <- filter(data_sell, year == 2020)\n",
    "data_sell_2021 <- filter(data_sell, year == 2021)\n",
    "\n",
    "# Set the final data that will be used in the model\n",
    "data_sell_final <- data_sell\n",
    "print(nrow(data_sell))\n",
    "print(nrow(data_sell_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Data Cleaning\n",
    "Defining a function to remove outliers from a DataFrame based on the interquartile range (IQR) method, specifically targeting the `price_m2` column. It calculates the IQR and uses it to filter out values that fall outside 1.5 times the IQR from the first and third quartiles. The filtered DataFrame is then returned, and the number of removed lines and final rows are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Fuction to remove outliers\n",
    "remove_outliers <- function(df, column_name) {\n",
    "\n",
    "  # Defining the DataFrame to be cleaned\n",
    "  df_filtered <- df\n",
    "\n",
    "  # Removing outliers from the DataFrame using the IQR method\n",
    "  q1_preco_mes <- quantile(df$price_m2, 0.25, na.rm = TRUE)\n",
    "  q3_preco_mes <- quantile(df$price_m2, 0.75, na.rm = TRUE)\n",
    "  iqr_preco_mes <- q3_preco_mes - q1_preco_mes\n",
    "  lower_bound_preco_mes <- q1_preco_mes - 1.5 * iqr_preco_mes\n",
    "  upper_bound_preco_mes <- q3_preco_mes + 1.5 * iqr_preco_mes\n",
    "\n",
    "  # Using the price per square meter as the reference for the filter\n",
    "  df_filtered <- df_filtered[df_filtered$price_m2 >= lower_bound_preco_mes & df_filtered$price_m2 <= upper_bound_preco_mes,]\n",
    "\n",
    "  return(df_filtered)\n",
    "}\n",
    "\n",
    "df_filtered <- remove_outliers(data_sell_final, \"price_m2\")\n",
    "print(paste(\"Number of removed lines:\", nrow(data_sell_final) - nrow(df_filtered)))\n",
    "print(paste(\"Number of final rows:\", nrow(df_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Create the spatial weights matrix\n",
    "The calculation of the `teleworkability` index assumes that individuals work and reside in the same district. To address this issue, the main model will be re-estimated with a spatial econometric specification. This specification will include a spatial lag of the `teleworkability` index vector, utilizing a spatial weight matrix `commute_matrix` constructed from inter-district commuting flows. This approach will help control for the differing locations of employment and residences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Importing the Commute Matrix and Normalizing the Data\n",
    "This code imports a commute matrix from a CSV file and normalizes it to calculate the proportion of commuters between locations. The normalization transforms each entry to represent the proportion of commuters from one location relative to the total number of commuters from that location. This provides a clearer view of commuting patterns and relative flows between areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Import the commute matrix\n",
    "commute_matrix <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/occupation/commute_matrix.csv\", dec = \".\", header = TRUE, sep = \",\") %>% as.matrix()\n",
    "\n",
    "# Normalize the data by dividing each row by the row sum to get the proportion of commuters\n",
    "commute_matrix_normalized <- sweep(commute_matrix, 1, rowSums(commute_matrix), \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Import the Teleworkability Index Vector for Each Year\n",
    "This code imports the teleworkability index data for each year from 2018 to 2021 from CSV files. It then converts the `weighted_avg_teleworkable_score` column of each imported matrix into a numeric vector. These vectors represent the teleworkability index for different districts in São Paulo for each respective year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Import the teleworkability index vector for each year and make it a numeric vector\n",
    "teleworkability_index_2018 <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/occupation/teleworkability_district_SP_2018.csv\", dec = \".\", header = TRUE, sep = \",\") %>%  as.matrix()\n",
    "teleworkability_vector_2018 <- as.numeric(teleworkability_index_2018 [,6])\n",
    "teleworkability_index_2019 <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/occupation/teleworkability_district_SP_2019.csv\", dec = \".\", header = TRUE, sep = \",\") %>% as.matrix()\n",
    "teleworkability_vector_2019 <- as.numeric(teleworkability_index_2019 [,6])\n",
    "teleworkability_index_2020 <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/occupation/teleworkability_district_SP_2020.csv\", dec = \".\", header = TRUE, sep = \",\") %>% as.matrix()\n",
    "teleworkability_vector_2020 <- as.numeric(teleworkability_index_2020 [,6])\n",
    "teleworkability_index_2021 <- read.csv(\"/Users/halefboukarim/Documents/halef-thesis/occupation/teleworkability_district_SP_2021.csv\", dec = \".\", header = TRUE, sep = \",\") %>% as.matrix()\n",
    "teleworkability_vector_2021 <- as.numeric(teleworkability_index_2021 [,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Including the spacial lag of the teleworkability index in the model\n",
    "This code calculates the `spatial lag of the teleworkability index` for each year from 2018 to 2021. It multiplies the `normalized commute matrix` by the `teleworkability vector` for each year, resulting in a new vector that represents the `spatially lagged teleworkability index`, accounting for the influence of commuting patterns between districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "spatial_lag_teleworkability_2018 <- commute_matrix_normalized %*% teleworkability_vector_2018\n",
    "spatial_lag_teleworkability_2019 <- commute_matrix_normalized %*% teleworkability_vector_2019\n",
    "spatial_lag_teleworkability_2020 <- commute_matrix_normalized %*% teleworkability_vector_2020\n",
    "spatial_lag_teleworkability_2021 <- commute_matrix_normalized %*% teleworkability_vector_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines the `teleworkability index's first and third` columns (ID and year) with the `spatially lagged` teleworkability index for each year from 2018 to 2021. It creates a data frame for each year by first extracting the relevant columns (ID and year) from the original teleworkability index data, then binding these columns with the spatially lagged teleworkability index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "teleworkability_index_2018_first_collunm <- as.data.frame(teleworkability_index_2018 [,c(1,3)])\n",
    "teleworkability_index_2018_matrix <- cbind(teleworkability_index_2018_first_collunm, spatial_lag_teleworkability_2018)\n",
    "colnames(teleworkability_index_2018_matrix) <- c(\"ID\", \"year\", \"teleworkability_adj\")\n",
    "teleworkability_index_2019_first_collunm <- as.data.frame(teleworkability_index_2019 [,c(1,3)])\n",
    "teleworkability_index_2019_matrix <- cbind(teleworkability_index_2019_first_collunm, spatial_lag_teleworkability_2019)\n",
    "colnames(teleworkability_index_2019_matrix) <- c(\"ID\", \"year\", \"teleworkability_adj\")\n",
    "teleworkability_index_2020_first_collunm <- as.data.frame(teleworkability_index_2020 [,c(1,3)])\n",
    "teleworkability_index_2020_matrix <- cbind(teleworkability_index_2020_first_collunm, spatial_lag_teleworkability_2020)\n",
    "colnames(teleworkability_index_2020_matrix) <- c(\"ID\", \"year\", \"teleworkability_adj\")\n",
    "teleworkability_index_2021_first_collunm <- as.data.frame(teleworkability_index_2021 [,c(1,3)])\n",
    "teleworkability_index_2021_matrix <- cbind(teleworkability_index_2021_first_collunm, spatial_lag_teleworkability_2021)\n",
    "colnames(teleworkability_index_2021_matrix) <- c(\"ID\", \"year\", \"teleworkability_adj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code combines the data frames containing the adjusted teleworkability index for each year from 2018 to 2021 into a single data frame. The `rbind` function is used to bind the rows of the yearly matrices together, and the resulting matrix is then converted into a data frame with `as.data.frame`. This consolidated data frame, `teleworkability_adj_matrix`, allows for comprehensive analysis across all years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "teleworkability_adj_matrix <- rbind(teleworkability_index_2018_matrix, teleworkability_index_2019_matrix, teleworkability_index_2020_matrix, teleworkability_index_2021_matrix)\n",
    "teleworkability_adj_matrix <- as.data.frame(teleworkability_adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares and merges two data frames by matching on the \"ID\" and \"year\" columns. First, it converts the \"ID\" and \"year\" columns in both `df_filtered` and `teleworkability_adj_matrix` to integer type. Then, it performs a left join to merge `teleworkability_adj_matrix` into `df_filtered` based on the \"ID\" and \"year\" columns, resulting in a combined data frame that includes the teleworkability adjusted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df_filtered$ID <- as.integer(df_filtered$ID)\n",
    "teleworkability_adj_matrix$ID <- as.integer(teleworkability_adj_matrix$ID)\n",
    "df_filtered$year <- as.integer(df_filtered$year)\n",
    "teleworkability_adj_matrix$year <- as.integer(teleworkability_adj_matrix$year)\n",
    "df_filtered <- left_join(df_filtered, teleworkability_adj_matrix, by = c(\"ID\", \"year\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Select and ensure all columns are numeric\n",
    "corr_data <- df_filtered[, (names(df_filtered) %in% c(\"price_m2\", \"area_m2\", \"bedrooms\", \"bathrooms\",\"teleworkability_adj\", \"metro_dist_km\", \"delta_cbd_farialima\", \"inequality_meter\", \"garage\", \"avg_salario_medio\" ))]\n",
    "\n",
    "# Convert all columns to numeric\n",
    "corr_data <- data.frame(lapply(corr_data, function(x) as.numeric(as.character(x))))\n",
    "\n",
    "# Compute correlation at 2 decimal places\n",
    "corr_matrix <- round(cor(corr_data, use = \"complete.obs\"), 2)\n",
    "ggcorrplot(corr_matrix, hc.order = TRUE, type = \"lower\", lab = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Defining the Model parameters\n",
    "This subsection defines the formula for the regression model, specifying `price_m2` as the dependent variable and various property features as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(df_filtered)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "params <- (price_real_month ~ teleworkability_adj +\n",
    "  area_m2 +\n",
    "  bedrooms +\n",
    "  bathrooms + \n",
    "  metro_dist_km +\n",
    "  delta_cbd_farialima +\n",
    "  inequality_meter +\n",
    "  garage +\n",
    "  avg_salario_medio)\n",
    "\n",
    "#data_sell_final_2018 <- filter(data_sell_final, year(date) == 2018)\n",
    "reg_OLS <- lm(params, data = df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Checking the results for the linear regression model\n",
    "Checking the results for the linear regression model to understand the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(reg_OLS)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Running the diagnostic tests\n",
    "Running the diagnostic tests to check the normality, heteroskedasticity, and autocorrelation of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "if (!requireNamespace(\"nortest\", quietly = TRUE)) {\n",
    "  install.packages(\"nortest\")\n",
    "}\n",
    "if (!requireNamespace(\"lmtest\", quietly = TRUE)) {\n",
    "  install.packages(\"lmtest\")\n",
    "}\n",
    "library(nortest)\n",
    "library(lmtest)\n",
    "\n",
    "# Perform Shapiro-Wilk test or Anderson-Darling test based on sample size\n",
    "residuals_best_model <- resid(reg_OLS)\n",
    "sample_size <- length(residuals_best_model)\n",
    "\n",
    "if (sample_size >= 3 && sample_size <= 5000) {\n",
    "  shapiro_test <- shapiro.test(residuals_best_model)\n",
    "  print(shapiro_test)\n",
    "  if (shapiro_test$p.value > 0.05) {\n",
    "    print(\"Pass: Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Fail: Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "} else {\n",
    "  ad_test <- ad.test(residuals_best_model)\n",
    "  print(ad_test)\n",
    "  if (ad_test$p.value > 0.05) {\n",
    "    print(\"Pass: Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Fail: Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Perform Breusch-Pagan test for heteroskedasticity\n",
    "bp_test <- bptest(reg_OLS)\n",
    "print(bp_test)\n",
    "if (bp_test$p.value < 0.05) {\n",
    "  print(\"Fail: Heteroskedasticity detected (reject H0).\")\n",
    "} else {\n",
    "  print(\"Pass: No heteroskedasticity detected (fail to reject H0).\")\n",
    "}\n",
    "\n",
    "# Perform Durbin-Watson test for autocorrelation\n",
    "dw_test <- dwtest(reg_OLS)\n",
    "print(dw_test)\n",
    "dw_stat <- dw_test$statistic\n",
    "if (dw_stat < 1.5) {\n",
    "  print(\"Fail: Positive autocorrelation detected.\")\n",
    "} else if (dw_stat > 2.5) {\n",
    "  print(\"Fail: Negative autocorrelation detected.\")\n",
    "} else {\n",
    "  print(\"Pass: No autocorrelation detected.\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calcule os resíduos do modelo\n",
    "residuals <- residuals(reg_OLS)\n",
    "\n",
    "# Para calcular Moran's I, precisamos de uma matriz de vizinhança (exemplo com vizinhos mais próximos)\n",
    "coords <- as.matrix(df_filtered[,c(\"longitude\", \"latitude\")])\n",
    "nb <- knn2nb(knearneigh(coords, k=4))\n",
    "listw <- nb2listw(nb, style=\"W\")\n",
    "\n",
    "# Teste de Moran's I\n",
    "moran_test <- moran.test(residuals, listw)\n",
    "print(moran_test)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moran.mc(df_filtered$price_real_month, listw, nsim=999)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lm.morantest(reg_OLS, listw)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Spatial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "reg_lag <- lagsarlm(params, data = df_filtered, listw = listw)\n",
    "summary(reg_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lag_effects <- impacts(reg_lag, listw = listw, R = 999)\n",
    "lag_effects"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reg_error <- errorsarlm(params, data = df_filtered, listw = listw)\n",
    "summary(reg_error)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cTSbgY539t4f",
    "GXkt_hv69ch5",
    "676-fZnQ98KY",
    "QayeFylW-F9O",
    "bZramecV-cbp"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
