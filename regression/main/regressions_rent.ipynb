{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and Loading Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Installing and Loading Required Packages\n",
    "This section installs and loads the essential R packages for data analysis and visualization. The `ggcorrplot` package creates correlation plots, `car` offers regression diagnostics, and `HH` provides additional statistical methods. The `tidyverse` is used for data manipulation, `ggplot2` for visualization, and both `gridExtra` and `cowplot` help arrange multiple plots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T03:21:34.591413Z",
     "start_time": "2024-06-10T03:20:42.991911Z"
    }
   },
   "source": [
    "install.packages(\"showtext\")\n",
    "install.packages(\"ggcorrplot\")\n",
    "install.packages(\"car\")\n",
    "install.packages(\"HH\")\n",
    "install.packages(\"lmtest\")\n",
    "install.packages(\"nortest\")\n",
    "# Load necessary libraries\n",
    "library(showtext)\n",
    "#library(tidyverse)\n",
    "#library(ggplot2)\n",
    "#library(gridExtra)\n",
    "#library(cowplot)\n",
    "library(ggcorrplot)\n",
    "library(car)\n",
    "library(HH)\n",
    "library(lmtest)\n",
    "library(nortest)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n",
      "Updating HTML index of packages in '.Library'\n",
      "\n",
      "Making 'packages.html' ...\n",
      " done\n",
      "\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Reading the Data\n",
    "Reading the data from the CSV file and displaying the first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "data_rent <- read.csv(\"/Users/karimmbk/Documents/halef-thesis/regression/resources/rent_2018_2021.csv\", dec = \".\", header = TRUE, sep = \";\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Convert categorical variables to factors (dummy variables)\n",
    "Converting categorical variables to factors to use them in the regression model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_rent$gym <- as.factor(data_rent$gym)\n",
    "data_rent$field_quadra <- as.factor(data_rent$field_quadra)\n",
    "data_rent$elevator <- as.factor(data_rent$elevator)\n",
    "data_rent$furnished <- as.factor(data_rent$furnished)\n",
    "data_rent$swimming_pool <- as.factor(data_rent$swimming_pool)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Convert date to date format\n",
    "Converting the date column to a date format to filter the data by year."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_rent$date <- as.Date(data_rent$date, format = \"%d/%m/%Y\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Specification and Fitting\n",
    " Analyzing the correlation between the variables to understand the relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corr_data <- data_rent[, (names(data_rent) %in% c(\"price_real_month\", \"area_m2\", \"bedrooms\", \"bathrooms\", \"garage\", \"condo_real\", \"metro_dist_km\", \"delta_cbd_farialima\", \"teleworkable\", \"inequality_meter\", \"suite\"))]\n",
    "\n",
    "# Compute correlation at 2 decimal places\n",
    "corr_matrix <- round(cor(corr_data), 2)\n",
    "ggcorrplot(corr_matrix, hc.order = TRUE, type = \"lower\", lab = TRUE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Defining the Model parameters\n",
    "This subsection defines the formula for the regression model, specifying `price_m2` as the dependent variable and various property features as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "params <- (price_real_month ~ area_m2 +\n",
    "  bedrooms +\n",
    "  metro_dist_km +\n",
    "  delta_cbd_farialima +\n",
    "  inequality_meter +\n",
    "  teleworkable +\n",
    "  garage +\n",
    "  gym +\n",
    "  field_quadra +\n",
    "  elevator +\n",
    "  furnished +\n",
    "  swimming_pool)\n",
    "\n",
    "data_rent_2021 <- filter(data_rent, year(date) == 2021)\n",
    "reg <- lm(params, data = data_rent_2021)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Checking the results for the linear regression model\n",
    "Checking the results for the linear regression model to understand the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary(reg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform Shapiro-Wilk test\n",
    "residuals_best_model <- resid(reg)\n",
    "sample_size <- length(residuals_best_model)\n",
    "\n",
    "if (sample_size >= 3 && sample_size <= 5000) {\n",
    "  shapiro_test <- shapiro.test(residuals_best_model)\n",
    "  print(shapiro_test)\n",
    "  if (shapiro_test$p.value > 0.01) {\n",
    "    print(\"Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "} else {\n",
    "  print(\"Sample size out of range for Shapiro-Wilk test, using Anderson-Darling test instead.\")\n",
    "  ad_test <- ad.test(residuals_best_model)\n",
    "  print(ad_test)\n",
    "  if (ad_test$p.value > 0.01) {\n",
    "    print(\"Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Perform Breusch-Pagan test\n",
    "bp_test <- bptest(reg)\n",
    "print(bp_test)\n",
    "if (bp_test$p.value < 0.01) {\n",
    "  print(\"Heteroskedasticity detected (reject H0).\")\n",
    "} else {\n",
    "  print(\"No heteroskedasticity detected (fail to reject H0).\")\n",
    "}\n",
    "\n",
    "# Perform Durbin-Watson test\n",
    "dw_test <- dwtest(reg)\n",
    "print(dw_test)\n",
    "dw_stat <- dw_test$statistic\n",
    "if (dw_stat < 1.5) {\n",
    "  print(\"Positive autocorrelation detected.\")\n",
    "} else if (dw_stat > 2.5) {\n",
    "  print(\"Negative autocorrelation detected.\")\n",
    "} else {\n",
    "  print(\"No autocorrelation detected.\")\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adicionar a fonte Palatino Linotype usando o caminho completo para o arquivo TTF\n",
    "# Substitua \"path/to/PalatinoLinotype.ttf\" pelo caminho real do arquivo TTF\n",
    "font_add(\"Palatino\", regular = \"/Users/karimmbk/Documents/halef-thesis/regression/resources/Palatino Linotype.ttf\")\n",
    "\n",
    "# Habilitar showtext\n",
    "showtext_auto()\n",
    "\n",
    "predicted<- predict(reg)\n",
    "residuals <- resid(reg)\n",
    "stand_predicted <- (predicted - mean(predicted)) / sd(predicted)\n",
    "stand_residuals <- (residuals - mean(residuals)) / sd(residuals)\n",
    "\n",
    "# Ajustar a fonte e o tamanho do texto\n",
    "par(family = \"Palatino\", cex = 1, mar = c(5, 5, 4, 2) + 0.1)  # Ajustar as margens\n",
    "\n",
    "# Plotar os resíduos padronizados\n",
    "plot(stand_predicted, stand_residuals, main = \"Standardized residuals plot\",\n",
    "     xlab = \"Standardized predicted value\", ylab = \"Standardized residuals\",\n",
    "     col = \"#EE9B00\", pch = 1)  # Ajustar a cor dos pontos e o estilo\n",
    "\n",
    "# Adicionar linhas de referência\n",
    "abline(0, 0, col = \"black\")\n",
    "abline(h = -2, col = \"#0A9396\")\n",
    "abline(h = 2, col = \"#0A9396\")\n",
    "\n",
    "# Calculate the total number of values within the interval [-2, 2]\n",
    "within_interval <- sum(stand_residuals >= -2 & stand_residuals <= 2)\n",
    "mtext(paste(\"Percentage of values between -2 and 2:\", round(within_interval / length(stand_residuals) * 100, 2), \"%\"), side = 3, line = 0.5, cex = 0.8)\n",
    "\n",
    "# Calculate the histogram data without plotting\n",
    "hist_data <- hist(stand_residuals, plot = FALSE)\n",
    "\n",
    "# Calculate the normal distribution density values\n",
    "x_values <- seq(min(stand_residuals), max(stand_residuals), length = 100)\n",
    "y_values <- dnorm(x_values, mean = mean(stand_residuals), sd = sd(stand_residuals))\n",
    "\n",
    "# Determine the y-axis limit\n",
    "#y_max <- max(c(hist_data$density, y_values))\n",
    "\n",
    "# Plotar o histograma com limite no eixo y, e ajustar a cor\n",
    "hist(stand_residuals, freq = FALSE, ylim = c(0, 0.5), xlim = c(-4, 4),\n",
    "     main = \"Histogram with Normal Curve\", xlab = \"Standardized Residuals\",\n",
    "    col = \"#EE9B00\", border = \"#FFFFFF\")  # Ajuste a cor do histograma e da borda\n",
    "\n",
    "# Adicionar a curva de distribuição normal ajustando a cor\n",
    "curve(dnorm(x, mean = mean(stand_residuals), sd = sd(stand_residuals)), add = TRUE, lwd = 1.5, col = \"#0A9396\")  # Ajuste a cor da curva"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finding the best model\n",
    "Creating a code to run the best model and generate the best combination of variables and transformations to generate the final regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Function to calculate the best model\n",
    "The function `calculate_best_model` calculates the best model by generating all possible combinations of transformations for the independent variables. The function returns the best model, the best combination of variables and transformations, and the best R² value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "library(nortest)  # Para o teste de Anderson-Darling\n",
    "library(lmtest)   # Para os testes de Breusch-Pagan e Durbin-Watson\n",
    "library(car)      # Para outras funções de diagnóstico de regressão\n",
    "\n",
    "calculate_best_model <- function(data, dependent_var, independent_vars, transformations) {\n",
    "  best_r2 <- -Inf\n",
    "  best_model <- NULL\n",
    "  best_combination <- NULL\n",
    "  list_r2 <- NULL\n",
    "\n",
    "  # Gerar todas as combinações de transformações para todas as variáveis independentes\n",
    "  trans_combinations <- expand.grid(rep(list(transformations), length(independent_vars)))\n",
    "  colnames(trans_combinations) <- independent_vars\n",
    "\n",
    "  for (trans_row in seq_len(nrow(trans_combinations))) {\n",
    "    transformed_data <- data\n",
    "    formula_parts <- c(dependent_var, \"~\")\n",
    "    for (var in independent_vars) {\n",
    "      trans <- trans_combinations[trans_row, var]\n",
    "      transformed_var <- switch(as.character(trans),\n",
    "                                \"X\" = data[[var]],\n",
    "                                \"1_X\" = 1 / data[[var]],\n",
    "                                \"LnX\" = log(data[[var]]),\n",
    "                                \"X__2\" = data[[var]]^2,\n",
    "                                \"X__1_2\" = sqrt(data[[var]]),\n",
    "                                \"1_X__2\" = 1 / (data[[var]]^2),\n",
    "                                \"1_X__1_2\" = 1 / sqrt(data[[var]]))\n",
    "      trans_name <- paste(var, trans, sep = \"_\")\n",
    "      transformed_data[[trans_name]] <- transformed_var\n",
    "      formula_parts <- c(formula_parts, trans_name)\n",
    "    }\n",
    "\n",
    "    formula_string <- paste(paste(formula_parts, collapse = \" + \"), \" + garage + gym + field_quadra + elevator + furnished + swimming_pool\")\n",
    "    formula_string <- gsub(\" \\\\+ ~ \\\\+ \", \" ~ \", formula_string)  # Corrigir formatação da fórmula\n",
    "    formula <- as.formula(formula_string)\n",
    "    model <- lm(formula, data = transformed_data)\n",
    "\n",
    "    r2 <- summary(model)$r.squared\n",
    "    list_r2 <- append(list_r2, r2)\n",
    "\n",
    "    # Verificar os testes de diagnóstico\n",
    "    ad_test <- ad.test(residuals(model))\n",
    "    bp_test <- bptest(model)\n",
    "    dw_test <- dwtest(model)\n",
    "\n",
    "    # Se os testes passarem, considerar o modelo\n",
    "    if (r2 > best_r2 &&\n",
    "      ad_test$p.value > 0.05 &&\n",
    "      bp_test$p.value > 0.05 &&\n",
    "      dw_test$statistic > 1.5 &&\n",
    "      dw_test$statistic < 2.5) {\n",
    "      best_r2 <- r2\n",
    "      best_model <- model\n",
    "      best_combination <- list(variables = independent_vars, transformations = as.list(trans_combinations[trans_row,]))\n",
    "    }\n",
    "  }\n",
    "  return(list(best_model = best_model, best_combination = best_combination, best_r2 = best_r2, list_r2 = list_r2))\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Defining the variables to be used in the model\n",
    "Defining the dependent and independent variables and transformations to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_rent$price_real_month <- log(data_rent$price_real_month)\n",
    "dependent_var <- \"price_real_month\"\n",
    "independent_vars <- c(\"area_m2\", \"bedrooms\", \"metro_dist_km\", \"delta_cbd_farialima\", \"inequality_meter\", \"teleworkable\")\n",
    "transformations <- c(\"X\", \"1_X\", \"LnX\", \"X__2\", \"X__1_2\", \"1_X__2\", \"1_X__1_2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Calculating the function to generate the best model\n",
    "Running the function to generate the best model and print the results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results <- calculate_best_model(data_rent, dependent_var, independent_vars, transformations)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Printing the result\n",
    "Printing the best model, the best combination of variables and transformations, and the best R² value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary(results$best_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform Shapiro-Wilk test\n",
    "residuals_best_model <- resid(results$best_model)\n",
    "sample_size <- length(residuals_best_model)\n",
    "\n",
    "if (sample_size >= 3 && sample_size <= 5000) {\n",
    "  shapiro_test <- shapiro.test(residuals_best_model)\n",
    "  print(shapiro_test)\n",
    "  if (shapiro_test$p.value > 0.01) {\n",
    "    print(\"Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "} else {\n",
    "  print(\"Sample size out of range for Shapiro-Wilk test, using Anderson-Darling test instead.\")\n",
    "  ad_test <- ad.test(residuals_best_model)\n",
    "  print(ad_test)\n",
    "  if (ad_test$p.value > 0.01) {\n",
    "    print(\"Residuals are normally distributed (fail to reject H0).\")\n",
    "  } else {\n",
    "    print(\"Residuals are not normally distributed (reject H0).\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Perform Breusch-Pagan test\n",
    "bp_test <- bptest(results$best_model)\n",
    "print(bp_test)\n",
    "if (bp_test$p.value < 0.01) {\n",
    "  print(\"Heteroskedasticity detected (reject H0).\")\n",
    "} else {\n",
    "  print(\"No heteroskedasticity detected (fail to reject H0).\")\n",
    "}\n",
    "\n",
    "# Perform Durbin-Watson test\n",
    "dw_test <- dwtest(results$best_model)\n",
    "print(dw_test)\n",
    "dw_stat <- dw_test$statistic\n",
    "if (dw_stat < 1.5) {\n",
    "  print(\"Positive autocorrelation detected.\")\n",
    "} else if (dw_stat > 2.5) {\n",
    "  print(\"Negative autocorrelation detected.\")\n",
    "} else {\n",
    "  print(\"No autocorrelation detected.\")\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "predicted<- predict(results$best_model)\n",
    "residuals <- resid(results$best_model)\n",
    "stand_predicted <- (predicted - mean(predicted)) / sd(predicted)\n",
    "stand_residuals <- (residuals - mean(residuals)) / sd(residuals)\n",
    "\n",
    "plot(stand_predicted, stand_residuals, main = \"Standardized residuals plot\", xlab = \"Standardized predicted value\", ylab = \"Standardized residuals\")\n",
    "abline(0, 0)\n",
    "abline(h = -2, col = \"red\")\n",
    "abline(h = 2, col = \"red\")\n",
    "\n",
    "# Calculate the total number of values within the interval [-2, 2]\n",
    "within_interval <- sum(stand_residuals >= -2 & stand_residuals <= 2)\n",
    "mtext(paste(\"Percentage of values between -2 and 2:\", round(within_interval / length(stand_residuals) * 100, 2), \"%\"), side = 3, line = 0.5, cex = 0.8)\n",
    "\n",
    "# Calculate the histogram data without plotting\n",
    "hist_data <- hist(stand_residuals, plot = FALSE)\n",
    "\n",
    "# Calculate the normal distribution density values\n",
    "x_values <- seq(min(stand_residuals), max(stand_residuals), length = 100)\n",
    "y_values <- dnorm(x_values, mean = mean(stand_residuals), sd = sd(stand_residuals))\n",
    "\n",
    "# Determine the y-axis limit\n",
    "y_max <- max(c(hist_data$density, y_values))\n",
    "\n",
    "# Plot the histogram with the y-axis limit\n",
    "hist(stand_residuals, freq = FALSE, ylim = c(0, y_max),\n",
    "     main = \"Histogram with Normal Curve\", xlab = \"Standardized Residuals\")\n",
    "\n",
    "# Add the normal distribution curve\n",
    "curve(dnorm(x, mean = mean(stand_residuals), sd = sd(stand_residuals)), add = TRUE, lwd = 2)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cTSbgY539t4f",
    "GXkt_hv69ch5",
    "676-fZnQ98KY",
    "QayeFylW-F9O",
    "bZramecV-cbp"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
